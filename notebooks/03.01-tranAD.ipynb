{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c723ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")  # Go up one level to the UROP directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db69208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters and settings\n",
    "SEED = 1\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c0c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import LazyWindowedDataset, train_test_split\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize Dataset\n",
    "full_train_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"train\",\n",
    "    anomaly_type=['normal'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=100,\n",
    "    stride_ms=50,\n",
    ")\n",
    "\n",
    "train_source_dataset, val_source_dataset = train_test_split(full_train_source_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "val_loader = DataLoader(val_source_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dceeb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model).float() * (-math.log(10000.0) / d_model))\n",
    "        pe += torch.sin(position * div_term)\n",
    "        pe += torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, pos=0):\n",
    "        x = x + self.pe[pos:pos+x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TranAD(nn.Module):\n",
    "\tdef __init__(self, feats, n_window):\n",
    "\t\tsuper(TranAD, self).__init__()\n",
    "\t\tself.name = 'TranAD' # unused\n",
    "\t\tself.lr = LR # unused\n",
    "\t\tself.batch = BATCH_SIZE # unused\n",
    "\n",
    "\t\tself.n_feats = feats\n",
    "\t\tself.n_window = n_window\n",
    "\t\t\n",
    "\t\tself.n = self.n_feats * self.n_window # unused\n",
    "\t\t\n",
    "\t\tself.pos_encoder = PositionalEncoding(2 * feats, 0.1, self.n_window)\n",
    "\t\t\n",
    "\t\tencoder_layers = TransformerEncoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(encoder_layers, 1)\n",
    "\t\t\n",
    "\t\tdecoder_layers1 = TransformerDecoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_decoder1 = TransformerDecoder(decoder_layers1, 1)\n",
    "\t\t\n",
    "\t\tdecoder_layers2 = TransformerDecoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_decoder2 = TransformerDecoder(decoder_layers2, 1)\n",
    "\t\t\n",
    "\t\tself.fcn = nn.Sequential(nn.Linear(2 * feats, feats), nn.Sigmoid())\n",
    "\n",
    "\tdef encode(self, src, c, tgt):\n",
    "\t\tsrc = torch.cat((src, c), dim=2)\n",
    "\t\tsrc = src * math.sqrt(self.n_feats)\n",
    "\t\tsrc = self.pos_encoder(src)\n",
    "\t\tmemory = self.transformer_encoder(src)\n",
    "\t\ttgt = tgt.repeat(1, 1, 2)\n",
    "\t\treturn tgt, memory\n",
    "\n",
    "\tdef forward(self, src, tgt):\n",
    "\t\t# Phase 1 - Without anomaly scores\n",
    "\t\tc = torch.zeros_like(src)\n",
    "\t\tx1 = self.fcn(self.transformer_decoder1(*self.encode(src, c, tgt)))\n",
    "\t\t# Phase 2 - With anomaly scores\n",
    "\t\tc = (x1 - src) ** 2\n",
    "\t\tx2 = self.fcn(self.transformer_decoder2(*self.encode(src, c, tgt)))\n",
    "\t\treturn x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d5c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/massimiliano/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_12404/44607485.py:58: UserWarning: TranAD does not have a 'get_config' method. Setting model_config to None.\n",
      "  warnings.warn(f\"{model.__class__.__name__} does not have a 'get_config' method. Setting model_config to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1600, 7])\n",
      "torch.Size([1600, 128, 7])\n",
      "torch.Size([1, 128, 7])\n",
      "128 7\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.54 GiB. GPU 0 has a total capacity of 7.64 GiB of which 6.52 GiB is free. Including non-PyTorch memory, this process has 280.00 MiB memory in use. Of the allocated memory 85.75 MiB is allocated by PyTorch, and 6.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     93\u001b[39m model = TranAD(\n\u001b[32m     94\u001b[39m     feats=\u001b[32m7\u001b[39m, \u001b[38;5;66;03m# number of features\u001b[39;00m\n\u001b[32m     95\u001b[39m     n_window=\u001b[32m1600\u001b[39m, \u001b[38;5;66;03m# window size\u001b[39;00m\n\u001b[32m     96\u001b[39m ).to(device)\n\u001b[32m     97\u001b[39m optimizer = AdamW(model.parameters(), lr=LR)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTranAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMSELoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(name, model, criterion, optimizer, train_loader, val_loader, start_epoch, num_epochs, save_every, save_dir, verbose)\u001b[39m\n\u001b[32m     65\u001b[39m start_time = time.time()\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Train for one epoch and evaluate on validation set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# val_loss, val_auc = evaluate(model, val_loader, criterion, ['mic', 'acc', 'gyro'], verbose)\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# End timing the epoch\u001b[39;00m\n\u001b[32m     72\u001b[39m epoch_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, verbose)\u001b[39m\n\u001b[32m     32\u001b[39m elem = elem.view(\u001b[32m1\u001b[39m, local_bs, feat_size)\n\u001b[32m     34\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass on [128, 1600, 7]\u001b[39;00m\n\u001b[32m     36\u001b[39m loss = criterion(outputs, inputs)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mTranAD.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt):\n\u001b[32m     56\u001b[39m \t\u001b[38;5;66;03m# Phase 1 - Without anomaly scores\u001b[39;00m\n\u001b[32m     57\u001b[39m \tc = torch.zeros_like(src)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \tx1 = \u001b[38;5;28mself\u001b[39m.fcn(\u001b[38;5;28mself\u001b[39m.transformer_decoder1(*\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     59\u001b[39m \t\u001b[38;5;66;03m# Phase 2 - With anomaly scores\u001b[39;00m\n\u001b[32m     60\u001b[39m \tc = (x1 - src) ** \u001b[32m2\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mTranAD.encode\u001b[39m\u001b[34m(self, src, c, tgt)\u001b[39m\n\u001b[32m     49\u001b[39m src = src * math.sqrt(\u001b[38;5;28mself\u001b[39m.n_feats)\n\u001b[32m     50\u001b[39m src = \u001b[38;5;28mself\u001b[39m.pos_encoder(src)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m memory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m tgt = tgt.repeat(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tgt, memory\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/transformer.py:517\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    514\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    525\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/transformer.py:920\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    916\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m._ff_block(\u001b[38;5;28mself\u001b[39m.norm2(x))\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    919\u001b[39m         x\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m         + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m     )\n\u001b[32m    922\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28mself\u001b[39m._ff_block(x))\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/transformer.py:934\u001b[39m, in \u001b[36mTransformerEncoderLayer._sa_block\u001b[39m\u001b[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sa_block\u001b[39m(\n\u001b[32m    928\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    929\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    932\u001b[39m     is_causal: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    933\u001b[39m ) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/activation.py:1373\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1347\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1348\u001b[39m         query,\n\u001b[32m   1349\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1370\u001b[39m         is_causal=is_causal,\n\u001b[32m   1371\u001b[39m     )\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/functional.py:6410\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6407\u001b[39m k = k.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m   6408\u001b[39m v = v.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m-> \u001b[39m\u001b[32m6410\u001b[39m attn_output = \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[32m   6412\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6413\u001b[39m attn_output = (\n\u001b[32m   6414\u001b[39m     attn_output.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001b[32m   6415\u001b[39m )\n\u001b[32m   6417\u001b[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 8.54 GiB. GPU 0 has a total capacity of 7.64 GiB of which 6.52 GiB is free. Including non-PyTorch memory, this process has 280.00 MiB memory in use. Of the allocated memory 85.75 MiB is allocated by PyTorch, and 6.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from src import train_model, adjust_time_series_size, minmax_normalize, save_model_checkpoint\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, verbose=False):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "\n",
    "    for batch_idx, (mic, acc, gyro, labels) in enumerate(dataloader):\n",
    "        # Normalize input tensors\n",
    "        time_size = mic.shape[1]\n",
    "        mic_norm = minmax_normalize(adjust_time_series_size(mic, time_size, 'repeat_start'))\n",
    "        acc_norm = minmax_normalize(adjust_time_series_size(acc, time_size, 'repeat_start'))\n",
    "        gyro_norm = minmax_normalize(adjust_time_series_size(gyro, time_size, 'repeat_start'))\n",
    "        feat_size = mic.shape[2] + acc.shape[2] + gyro.shape[2]\n",
    "\n",
    "        # Concatenate inputs\n",
    "        inputs = torch.cat([mic_norm, acc_norm, gyro_norm], dim=2).to(device)\n",
    "        print(inputs.shape)\n",
    "\n",
    "        local_bs = inputs.shape[0]\n",
    "        window = inputs.permute(1, 0, 2)\n",
    "        print(window.shape)\n",
    "        elem = window[-1, :, :].unsqueeze(dim=0)\n",
    "        print(elem.shape)\n",
    "        print(local_bs, feat_size)\n",
    "        elem = elem.view(1, local_bs, feat_size)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(window, elem)  # Forward pass on [128, 1600, 7]\n",
    "        loss = criterion(outputs, inputs)  # Compute loss\n",
    "        print(loss.shape)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(dataloader)} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return running_loss / len(dataloader)  # Return average loss for the epoch\n",
    "\n",
    "def train_model(name, model, criterion, optimizer, train_loader, val_loader,\n",
    "                start_epoch=0, num_epochs=10, save_every=1, \n",
    "                save_dir='checkpoints', verbose=True):\n",
    "    \"\"\"Function to train the model for multiple epochs\"\"\"\n",
    "    train_losses, val_losses, val_aucs = [], [], []  # Initialize lists to store metrics\n",
    "\n",
    "    # Get model config\n",
    "    if hasattr(model, 'get_config'):\n",
    "        model_config = model.get_config()\n",
    "    else:\n",
    "        warnings.warn(f\"{model.__class__.__name__} does not have a 'get_config' method. Setting model_config to None.\")\n",
    "        model_config = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        adjusted_current_epoch = start_epoch + epoch\n",
    "\n",
    "        # Start timing the epoch\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train for one epoch and evaluate on validation set\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, verbose)\n",
    "        # val_loss, val_auc = evaluate(model, val_loader, criterion, ['mic', 'acc', 'gyro'], verbose)\n",
    "\n",
    "        # End timing the epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "        eta = epoch_time * (num_epochs - epoch - 1)\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        # val_losses.append(val_loss)\n",
    "        # val_aucs.append(val_auc)\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] (Checkpoint Epoch: {adjusted_current_epoch + 1}) | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val AUC: {val_auc:.4f}\")\n",
    "        print(f\"Time Spent: {epoch_time:.2f}s | ETA: {eta:.2f}s | Current Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "\n",
    "        if (save_every and (epoch + 1) % save_every == 0) or (epoch == num_epochs - 1):\n",
    "            # Save model checkpoint periodically with adjusted epoch\n",
    "            save_model_checkpoint(save_dir, name, model, model_config, optimizer,\n",
    "                                  adjusted_current_epoch + 1, train_losses, val_losses, val_aucs)\n",
    "\n",
    "    return model, train_losses, val_losses, val_aucs  # Return trained model and metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TranAD(\n",
    "    feats=7, # number of features\n",
    "    n_window=1600, # window size\n",
    ").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "train_model(\n",
    "    name=\"TranAD\",\n",
    "    model=model,\n",
    "    criterion=MSELoss,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,\n",
    "    start_epoch=0,\n",
    "    num_epochs=5,\n",
    "    save_every=5,\n",
    "    save_dir='checkpoints',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ba690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
