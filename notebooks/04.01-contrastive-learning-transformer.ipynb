{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")  # Go up one level to the UROP directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8439a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEED = 1\n",
    "TRAIN_BATCH_SIZE = 64 # on-line learning\n",
    "TEST_BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-3\n",
    "\n",
    "# Model hyper-parameters\n",
    "TF_DROPOUT = 0.1\n",
    "D_MODEL = 64\n",
    "N_HEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DIM_FF = 128\n",
    "DIM_OUT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8527d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from src import LazyWindowedDataset, train_test_split\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize Dataset\n",
    "full_train_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"train\",\n",
    "    anomaly_type=['normal'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=100,\n",
    "    stride_ms=50,\n",
    ")\n",
    "\n",
    "train_source_dataset, val_source_dataset = train_test_split(full_train_source_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_source_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "val_loader = DataLoader(val_source_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)\n",
    "\n",
    "test_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"test\",\n",
    "    anomaly_type=['normal', 'anomaly'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=100,\n",
    "    stride_ms=50,\n",
    ")\n",
    "test_loader = DataLoader(test_source_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c7e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedContrastiveModel(nn.Module):\n",
    "    def __init__(self, transformer_model, dim_ff, dim_out):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model  # Your RoPeTimeSeriesTransformer\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(transformer_model.config['d_model'], dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_masked, x_masked_alt):\n",
    "        # Encode both masked views\n",
    "        emb1 = self.transformer(x_masked)    # shape: [B, T, input_dim]\n",
    "        emb2 = self.transformer(x_masked_alt)\n",
    "\n",
    "        # Pool embeddings, (mean over time)\n",
    "        emb1 = emb1.mean(dim=1)  # [B, input_dim]\n",
    "        emb2 = emb2.mean(dim=1)\n",
    "\n",
    "        # Project to lower dim for contrastive loss\n",
    "        z1 = self.projector(emb1)  # [B, dim_out]\n",
    "        z2 = self.projector(emb2)\n",
    "\n",
    "        return z1, z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from src.save import save_model_checkpoint\n",
    "from src.utils import adjust_time_series_size, stack_on_last_dim, z_score_normalize\n",
    "\n",
    "\n",
    "def jitter(x, sigma=0.1):\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    return x + sigma * torch.randn_like(x)\n",
    "\n",
    "def zero_mask(x, mask_ratio=0.1):\n",
    "    \"\"\"Randomly zero out segments\"\"\"\n",
    "    B, T, D = x.shape\n",
    "    mask = torch.rand(B, T, 1, device=x.device) < mask_ratio\n",
    "    return x.masked_fill(mask, 0.)\n",
    "\n",
    "def create_views(x, augmentation_fns):\n",
    "    \"\"\"\n",
    "    Given input x [B,T,D], create two augmented views\n",
    "    \"\"\"\n",
    "    view1 = x.clone()\n",
    "    view2 = x.clone()\n",
    "    for fn in augmentation_fns:\n",
    "        view1 = fn(view1)\n",
    "        view2 = fn(view2)\n",
    "    return view1, view2\n",
    "\n",
    "def info_nce_loss(z1, z2, temperature=0.5):\n",
    "    \"\"\"Compute InfoNCE contrastive loss\"\"\"\n",
    "    z1 = nn.functional.normalize(z1, dim=1)\n",
    "    z2 = nn.functional.normalize(z2, dim=1)\n",
    "    logits = torch.matmul(z1, z2.T) / temperature  # [B, B]\n",
    "    targets = torch.arange(z1.size(0), device=z1.device)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch_contrastive(\n",
    "        model, dataloader, optimizer, \n",
    "        temperature=0.5,\n",
    "        augmentations=(jitter, zero_mask),\n",
    "        verbose=False\n",
    "    ):\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    iterator = dataloader\n",
    "    if verbose:\n",
    "        iterator = tqdm(dataloader, desc=\"Contrastive Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (mic, acc, gyro, _) in enumerate(iterator):\n",
    "        \n",
    "        # prepare input [B,T,D]\n",
    "        acc_adjusted = adjust_time_series_size(acc, mic.shape[1], 'resample')\n",
    "        gyro_adjusted = adjust_time_series_size(gyro, mic.shape[1], 'resample')\n",
    "\n",
    "         # Normalize input tensors\n",
    "        mic_norm = z_score_normalize(mic)\n",
    "        acc_norm = z_score_normalize(acc_adjusted)\n",
    "        gyro_norm = z_score_normalize(gyro_adjusted)\n",
    "\n",
    "        # Stack inputs along the feature dimension\n",
    "        x = stack_on_last_dim(mic_norm, acc_norm, gyro_norm).to(device)\n",
    "\n",
    "        # create two views\n",
    "        v1, v2 = create_views(x, augmentations) \n",
    "\n",
    "        # encode\n",
    "        z1, z2 = model(v1, v2)  # # [B, dim_out]\n",
    "        \n",
    "        # contrastive loss\n",
    "        loss = info_nce_loss    (z1, z2, temperature)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return running_loss / total\n",
    "\n",
    "def train_model_contrastive(\n",
    "        name, model, optimizer, train_loader,\n",
    "        temperature=0.5,\n",
    "        num_epochs=10,\n",
    "        save_every=1, \n",
    "        verbose=False\n",
    "    ):\n",
    "    train_losses = []\n",
    "    # no val loop for contrastive pretrain\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        start = time.time()\n",
    "        loss = train_one_epoch_contrastive(model, train_loader, optimizer,\n",
    "                                           temperature,\n",
    "                                           verbose=verbose)\n",
    "        train_losses.append(loss)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | Contrastive Loss: {loss:.4f} | Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            save_model_checkpoint(name + '_contrastive', model, None, optimizer,\n",
    "                                  epoch, [loss], [], [])\n",
    "    return model, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd8222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RoPeTimeSeriesTransformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    'input_dim': 7,\n",
    "    'd_model': D_MODEL,\n",
    "    'nhead': N_HEAD,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'dim_feedforward': DIM_FF,\n",
    "    'dropout': TF_DROPOUT,\n",
    "}\n",
    "transformer = RoPeTimeSeriesTransformer.from_config(config).to(device)\n",
    "constrastive_model = MaskedContrastiveModel(transformer_model=transformer, dim_ff=DIM_FF, dim_out=32).to(device)\n",
    "contrastive_optimizer = torch.optim.AdamW(constrastive_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Contrastive Training:   0%|          | 0/3425 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Contrastive Training:   1%|          | 20/3425 [00:44<2:05:12,  2.21s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted by user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_model_contrastive(\n",
    "        name='aerodactyl', \n",
    "        model=constrastive_model, \n",
    "        optimizer=contrastive_optimizer, \n",
    "        train_loader=train_loader,\n",
    "        temperature=0.5,\n",
    "        num_epochs=1, \n",
    "        save_every=1, \n",
    "        verbose=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d767475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedTransformerModel(nn.Module):\n",
    "    def __init__(self, pretrained_transformer, dim_ff, dim_out):\n",
    "        super().__init__()\n",
    "        self.transformer = pretrained_transformer\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(pretrained_transformer.config['d_model'], dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the transformer\n",
    "        emb = self.transformer(x)  # shape: [B, T, d_model]\n",
    "\n",
    "        # Pass through the projection head\n",
    "        output = self.projection_head(emb)  # shape: [B, T, dim_out]\n",
    "\n",
    "        return output\n",
    "    \n",
    "model = PretrainedTransformerModel(pretrained_transformer=transformer, dim_ff=DIM_FF, dim_out=7).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e85ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "PretrainedTransformerModel                         [64, 1600, 7]             --\n",
       "├─RoPeTimeSeriesTransformer: 1-1                   [64, 1600, 64]            --\n",
       "│    └─HybridSensorPositionalEncoding: 2-1         [64, 1600, 7]             --\n",
       "│    └─Linear: 2-2                                 [64, 1600, 64]            512\n",
       "│    └─TransformerEncoder: 2-3                     [64, 1600, 64]            --\n",
       "│    │    └─ModuleList: 3-1                        --                        133,888\n",
       "├─Sequential: 1-2                                  [64, 1600, 7]             --\n",
       "│    └─Linear: 2-4                                 [64, 1600, 128]           8,320\n",
       "│    └─ReLU: 2-5                                   [64, 1600, 128]           --\n",
       "│    └─Linear: 2-6                                 [64, 1600, 7]             903\n",
       "====================================================================================================\n",
       "Total params: 143,623\n",
       "Trainable params: 143,623\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.93\n",
       "====================================================================================================\n",
       "Input size (MB): 2.87\n",
       "Forward/backward pass size (MB): 1211.60\n",
       "Params size (MB): 0.31\n",
       "Estimated Total Size (MB): 1214.77\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(TRAIN_BATCH_SIZE, 1600, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/massimiliano/Projects/UROP/src/train.py:186: UserWarning: PretrainedTransformerModel does not have a 'get_config' method. Setting model_config to None.\n",
      "  warnings.warn(f\"{model.__class__.__name__} does not have a 'get_config' method. Setting model_config to None.\")\n",
      "Training:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.34s/batch]\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00,  1.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] (Checkpoint Epoch: 1) | Train Loss: 0.021971 | Val Loss: 1.007521 | Val AUC: nan\n",
      "Time Spent: 1.89s | ETA: 0.00s | Current Time: 2025-05-15 14:29:36\n",
      "Checkpoint saved at checkpoints/absol_PretrainedTransformerModel_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src import train_model, evaluate\n",
    "\n",
    "try:\n",
    "    train_model(\n",
    "        name='aerodactyl',\n",
    "        model=model, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, # Skip validation to speed up\n",
    "        merge_startegy='stack',\n",
    "        num_epochs=1, \n",
    "        verbose=1,\n",
    "        train_num_batches=50,\n",
    "        val_num_batches=50,\n",
    "        save_every=1,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c897e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:  33%|███▎      | 176/540 [01:21<02:48,  2.16batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m loss, auc = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstack\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall S+T | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/UROP/src/train.py:135\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader, criterion, sensors_to_test, merge_strategy, num_batches, verbose)\u001b[39m\n\u001b[32m    133\u001b[39m outputs = model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    134\u001b[39m loss = criterion(inputs, outputs)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m val_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Accumulate validation loss\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Compute reconstruction errors for each sample\u001b[39;00m\n\u001b[32m    138\u001b[39m batch_errors = ((inputs - outputs) ** \u001b[32m2\u001b[39m).mean(dim=\u001b[32m1\u001b[39m).cpu().numpy()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss, auc = evaluate(model, test_loader, criterion, merge_strategy='stack', verbose=True)\n",
    "print(f\"Overall S+T | Loss: {loss:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edabdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
