{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4955a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work based on \"TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Dat\" by Tuli et. al. https://arxiv.org/abs/2201.07284\n",
    "# Original implementation https://github.com/imperial-qore/TranAD\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")  # Go up one level to the UROP directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8439a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEED = 1\n",
    "TRAIN_BATCH_SIZE = 1 # on-line learning\n",
    "TEST_BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-3\n",
    "\n",
    "# Model hyper-parameters\n",
    "PE_DROPOUT = 0.1\n",
    "TF_DROPOUT = 0.1\n",
    "D_MODEL = 64\n",
    "N_HEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DIM_FF = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8527d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import LazyWindowedDataset, train_test_split\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize Dataset\n",
    "full_train_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"train\",\n",
    "    anomaly_type=['normal'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=100,\n",
    "    stride_ms=50,\n",
    ")\n",
    "\n",
    "train_source_dataset, val_source_dataset = train_test_split(full_train_source_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_source_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "val_loader = DataLoader(val_source_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)\n",
    "\n",
    "test_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"test\",\n",
    "    anomaly_type=['normal', 'anomaly'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=100,\n",
    "    stride_ms=50,\n",
    ")\n",
    "test_loader = DataLoader(test_source_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfb913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HybridSensorPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len=6000):\n",
    "        super().__init__()\n",
    "        rope_dim = 6  # acc + gyro\n",
    "        assert rope_dim % 2 == 0, \"RoPE dimension must be even\"\n",
    "\n",
    "        self.rope_dim = rope_dim\n",
    "\n",
    "        # RoPE frequency buffers\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, rope_dim, 2).float() / rope_dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)  # [rope_dim/2]\n",
    "\n",
    "        t = torch.arange(max_len, dtype=torch.float32)  # [T]\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)    # [T, rope_dim/2]\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs))  # [T, rope_dim/2]\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs))  # [T, rope_dim/2]\n",
    "\n",
    "        # Sinusoidal positional bias for mic\n",
    "        div_term = 1.0 / (10000 ** 0.0)  # scalar mic dim\n",
    "        pe = torch.sin(t.unsqueeze(1) * div_term)  # [T, 1]\n",
    "        self.register_buffer('mic_pe', pe)  # [T, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        assert D == 7, \"Expected input shape [B, T, 7]\"\n",
    "\n",
    "        # Split based on new order\n",
    "        x_mic = x[:, :, 0:1]      # [B, T, 1]\n",
    "        x_motion = x[:, :, 1:7]   # [B, T, 6]\n",
    "\n",
    "        # RoPE on motion\n",
    "        x1 = x_motion[..., ::2]  # [B, T, 3]\n",
    "        x2 = x_motion[..., 1::2] # [B, T, 3]\n",
    "        cos = self.cos_cached[:T].unsqueeze(0)  # [1, T, 3]\n",
    "        sin = self.sin_cached[:T].unsqueeze(0)  # [1, T, 3]\n",
    "\n",
    "        x_rotated = torch.cat([\n",
    "            x1 * cos - x2 * sin,\n",
    "            x1 * sin + x2 * cos\n",
    "        ], dim=-1)  # [B, T, 6]\n",
    "\n",
    "        # Positional bias on mic\n",
    "        x_mic_pe = x_mic + self.mic_pe[:T].unsqueeze(0)  # [B, T, 1]\n",
    "\n",
    "        # Reconstruct original feature order\n",
    "        return torch.cat([x_mic_pe, x_rotated], dim=-1)  # [B, T, 7]\n",
    "\n",
    "class RoPeTimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1, pe_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = HybridSensorPositionalEncoding()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, input_dim)\n",
    "\n",
    "        self.config = {\n",
    "            'input_dim': input_dim,\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_layers,\n",
    "            'dim_feedforward': dim_feedforward,\n",
    "            'dropout': dropout,\n",
    "            'pe_dropout': pe_dropout\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self,):\n",
    "        return self.config\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        return RoPeTimeSeriesTransformer(\n",
    "            input_dim=config['input_dim'],\n",
    "            d_model=config.get('d_model', 64),\n",
    "            nhead=config.get('nhead', 4),\n",
    "            num_layers=config.get('num_layers', 3),\n",
    "            dim_feedforward=config.get('dim_feedforward', 128),\n",
    "            dropout=config.get('dropout', 0.1),\n",
    "            pe_dropout=config.get('pe_dropout', 0.1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd8222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_dim': 7,\n",
    "    'd_model': D_MODEL,\n",
    "    'nhead': N_HEAD,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'dim_feedforward': DIM_FF,\n",
    "    'dropout': TF_DROPOUT,\n",
    "    'pe_dropout': PE_DROPOUT\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RoPeTimeSeriesTransformer.from_config(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "criterior = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72f5d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "RoPeTimeSeriesTransformer                     [1, 1600, 7]              --\n",
       "├─HybridSensorPositionalEncoding: 1-1         [1, 1600, 7]              --\n",
       "├─Linear: 1-2                                 [1, 1600, 64]             512\n",
       "├─TransformerEncoder: 1-3                     [1, 1600, 64]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 1600, 64]             33,472\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 1600, 64]             33,472\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [1, 1600, 64]             33,472\n",
       "│    │    └─TransformerEncoderLayer: 3-4      [1, 1600, 64]             33,472\n",
       "├─Linear: 1-4                                 [1, 1600, 7]              455\n",
       "===============================================================================================\n",
       "Total params: 134,855\n",
       "Trainable params: 134,855\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.07\n",
       "===============================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 17.29\n",
       "Params size (MB): 0.27\n",
       "Estimated Total Size (MB): 17.61\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(TRAIN_BATCH_SIZE, 1600, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed62ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [00:01<00:00, 35.10batch/s]\n",
      "Evaluation: 100%|██████████| 50/50 [00:00<00:00, 80.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] (Checkpoint Epoch: 1) | Train Loss: 0.837158 | Val Loss: 0.760757 | Val AUC: nan\n",
      "Time Spent: 2.05s | ETA: 0.00s | Current Time: 2025-05-15 11:12:13\n",
      "Checkpoint saved at checkpoints/abra_RoPeTimeSeriesTransformer_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src import train_model, evaluate\n",
    "\n",
    "try:\n",
    "    train_model(\n",
    "        name='abra', \n",
    "        model=model, \n",
    "        criterion=criterior, \n",
    "        optimizer=optimizer, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, # Skip validation to speed up\n",
    "        merge_startegy='stack',\n",
    "        num_epochs=1, \n",
    "        verbose=1,\n",
    "        train_num_batches=50,\n",
    "        val_num_batches=50,\n",
    "        save_every=1,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe6e084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 540/540 [04:04<00:00,  2.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall S+T | Loss: 0.7597, AUC: 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, auc = evaluate(model, test_loader, criterior, merge_strategy='stack', verbose=True)\n",
    "print(f\"Overall S+T | Loss: {loss:.4f}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bd992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
