{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work based on \"TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Dat\" by Tuli et. al. https://arxiv.org/abs/2201.07284\n",
    "# Original implementation https://github.com/imperial-qore/TranAD\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")  # Go up one level to the UROP directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db69208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters and settings\n",
    "SEED = 1\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c0c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import LazyWindowedDataset, train_test_split\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize Dataset\n",
    "full_train_source_dataset = LazyWindowedDataset(\n",
    "    root_dir=\"datasets/RoboticArm\",\n",
    "    split=\"train\",\n",
    "    anomaly_type=['normal'],\n",
    "    domain_type=['source', 'target'],\n",
    "    window_size_ms=50,\n",
    "    stride_ms=50,\n",
    ")\n",
    "\n",
    "train_source_dataset, val_source_dataset = train_test_split(full_train_source_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "val_loader = DataLoader(val_source_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dceeb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model).float() * (-math.log(10000.0) / d_model))\n",
    "        pe += torch.sin(position * div_term)\n",
    "        pe += torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, pos=0):\n",
    "        x = x + self.pe[pos:pos+x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TranAD(nn.Module):\n",
    "\tdef __init__(self, feats, n_window):\n",
    "\t\tsuper(TranAD, self).__init__()\n",
    "\t\tself.name = 'TranAD' # unused\n",
    "\t\tself.lr = LR # unused\n",
    "\t\tself.batch = BATCH_SIZE # unused\n",
    "\n",
    "\t\tself.n_feats = feats\n",
    "\t\tself.n_window = n_window\n",
    "\t\t\n",
    "\t\tself.n = self.n_feats * self.n_window # unused\n",
    "\t\t\n",
    "\t\tself.pos_encoder = PositionalEncoding(2 * feats, 0.1, self.n_window)\n",
    "\t\t\n",
    "\t\tencoder_layers = TransformerEncoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_encoder = TransformerEncoder(encoder_layers, 1)\n",
    "\t\t\n",
    "\t\tdecoder_layers1 = TransformerDecoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_decoder1 = TransformerDecoder(decoder_layers1, 1)\n",
    "\t\t\n",
    "\t\tdecoder_layers2 = TransformerDecoderLayer(d_model=2 * feats, nhead=feats, dim_feedforward=16, dropout=0.1)\n",
    "\t\tself.transformer_decoder2 = TransformerDecoder(decoder_layers2, 1)\n",
    "\t\t\n",
    "\t\tself.fcn = nn.Sequential(nn.Linear(2 * feats, feats), nn.Sigmoid())\n",
    "\n",
    "\tdef encode(self, src, c, tgt):\n",
    "\t\tsrc = torch.cat((src, c), dim=2)\n",
    "\t\tsrc = src * math.sqrt(self.n_feats)\n",
    "\t\tsrc = self.pos_encoder(src)\n",
    "\t\tmemory = self.transformer_encoder(src)\n",
    "\t\ttgt = tgt.repeat(1, 1, 2)\n",
    "\t\treturn tgt, memory\n",
    "\n",
    "\tdef forward(self, src, tgt):\n",
    "\t\t# Phase 1 - Without anomaly scores\n",
    "\t\tc = torch.zeros_like(src)\n",
    "\t\tx1 = self.fcn(self.transformer_decoder1(*self.encode(src, c, tgt)))\n",
    "\t\t# Phase 2 - With anomaly scores\n",
    "\t\tc = (x1 - src) ** 2\n",
    "\t\tx2 = self.fcn(self.transformer_decoder2(*self.encode(src, c, tgt)))\n",
    "\t\treturn x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d5c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/massimiliano/miniconda3/envs/urop/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_39946/3687020502.py:65: UserWarning: TranAD does not have a 'get_config' method. Setting model_config to None.\n",
      "  warnings.warn(f\"{model.__class__.__name__} does not have a 'get_config' method. Setting model_config to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs)=<class 'tuple'>\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 7.65 GiB of which 980.94 MiB is free. Including non-PyTorch memory, this process has 5.90 GiB memory in use. Of the allocated memory 5.70 GiB is allocated by PyTorch, and 18.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    105\u001b[39m optimizer = AdamW(model.parameters(), lr=LR)\n\u001b[32m    106\u001b[39m criterion = nn.MSELoss(reduction=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTranAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(name, model, criterion, optimizer, train_loader, val_loader, start_epoch, num_epochs, save_every, save_dir, verbose)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Train for one epoch and evaluate on validation set\u001b[39;00m\n\u001b[32m     75\u001b[39m n = epoch + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# val_loss, val_auc = evaluate(model, val_loader, criterion, ['mic', 'acc', 'gyro'], verbose)\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# End timing the epoch\u001b[39;00m\n\u001b[32m     80\u001b[39m epoch_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, n, dataloader, optimizer, criterion, verbose)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m#loss = criterion(outputs, inputs)  # Compute loss\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m#print(loss.shape)\u001b[39;00m\n\u001b[32m     44\u001b[39m loss = l1.mean()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     46\u001b[39m optimizer.step()  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[32m     48\u001b[39m running_loss += loss.item()  \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/urop/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 7.65 GiB of which 980.94 MiB is free. Including non-PyTorch memory, this process has 5.90 GiB memory in use. Of the allocated memory 5.70 GiB is allocated by PyTorch, and 18.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from src import train_model, adjust_time_series_size, minmax_normalize, save_model_checkpoint\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_one_epoch(model, n, dataloader, optimizer, criterion, verbose=False):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    \n",
    "    l1s = []\n",
    "    for batch_idx, (mic, acc, gyro, labels) in enumerate(dataloader):\n",
    "        # Normalize input tensors\n",
    "        time_size = mic.shape[1]\n",
    "        mic_norm = minmax_normalize(adjust_time_series_size(mic, time_size, 'repeat_start'))\n",
    "        acc_norm = minmax_normalize(adjust_time_series_size(acc, time_size, 'repeat_start'))\n",
    "        gyro_norm = minmax_normalize(adjust_time_series_size(gyro, time_size, 'repeat_start'))\n",
    "        feat_size = mic.shape[2] + acc.shape[2] + gyro.shape[2]\n",
    "\n",
    "        # Concatenate inputs\n",
    "        inputs = torch.cat([mic_norm, acc_norm, gyro_norm], dim=2).to(device)\n",
    "        # print(inputs.shape)\n",
    "\n",
    "        local_bs = inputs.shape[0]\n",
    "        window = inputs.permute(1, 0, 2)\n",
    "        # print(window.shape)\n",
    "        elem = window[-1, :, :].unsqueeze(dim=0)\n",
    "        # print(elem.shape)\n",
    "        # print(local_bs, feat_size)\n",
    "        elem = elem.view(1, local_bs, feat_size)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(window, elem)  # Forward pass on [128, 1600, 7]\n",
    "        print(f\"{type(outputs)=}\")\n",
    "        # print(f\"{outputs.shape=}\")\n",
    "        # print(f\"{type(inputs)}, {inputs.shape}\")\n",
    "        l1 = criterion(outputs, elem) if not isinstance(outputs, tuple) else (1 / n) * criterion(outputs[0], elem) + (1 - 1/n) * criterion(outputs[1], elem)\n",
    "        l1s.append(torch.mean(l1).item())\n",
    "        #loss = criterion(outputs, inputs)  # Compute loss\n",
    "        #print(loss.shape)\n",
    "        loss = l1.mean()\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(dataloader)} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return running_loss / len(dataloader)  # Return average loss for the epoch\n",
    "\n",
    "def train_model(name, model, criterion, optimizer, train_loader, val_loader,\n",
    "                start_epoch=0, num_epochs=10, save_every=1, \n",
    "                save_dir='checkpoints', verbose=True):\n",
    "    \"\"\"Function to train the model for multiple epochs\"\"\"\n",
    "    train_losses, val_losses, val_aucs = [], [], []  # Initialize lists to store metrics\n",
    "\n",
    "    # Get model config\n",
    "    if hasattr(model, 'get_config'):\n",
    "        model_config = model.get_config()\n",
    "    else:\n",
    "        warnings.warn(f\"{model.__class__.__name__} does not have a 'get_config' method. Setting model_config to None.\")\n",
    "        model_config = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        adjusted_current_epoch = start_epoch + epoch\n",
    "\n",
    "        # Start timing the epoch\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train for one epoch and evaluate on validation set\n",
    "        n = epoch + 1\n",
    "        train_loss = train_one_epoch(model, n, train_loader, optimizer, criterion, verbose)\n",
    "        # val_loss, val_auc = evaluate(model, val_loader, criterion, ['mic', 'acc', 'gyro'], verbose)\n",
    "\n",
    "        # End timing the epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "        eta = epoch_time * (num_epochs - epoch - 1)\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        # val_losses.append(val_loss)\n",
    "        # val_aucs.append(val_auc)\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] (Checkpoint Epoch: {adjusted_current_epoch + 1}) | Train Loss: {train_loss:.6f}\")# | Val Loss: {val_loss:.6f} | Val AUC: {val_auc:.4f}\")\n",
    "        print(f\"Time Spent: {epoch_time:.2f}s | ETA: {eta:.2f}s | Current Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "\n",
    "        if (save_every and (epoch + 1) % save_every == 0) or (epoch == num_epochs - 1):\n",
    "            # Save model checkpoint periodically with adjusted epoch\n",
    "            save_model_checkpoint(save_dir, name, model, model_config, optimizer,\n",
    "                                  adjusted_current_epoch + 1, train_losses, val_losses, val_aucs)\n",
    "\n",
    "    return model, train_losses, val_losses, val_aucs  # Return trained model and metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TranAD(\n",
    "    feats=7,        # number of features\n",
    "    n_window=1600,  # window size\n",
    ").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "train_model(\n",
    "    name=\"TranAD\",\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,\n",
    "    start_epoch=0,\n",
    "    num_epochs=5,\n",
    "    save_every=5,\n",
    "    save_dir='checkpoints',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ba690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
